type: gpt2
vocab_size: 69120
# max_seq_length: 2048
# n_positions: 1024
n_embd: 512
n_layer: 12
n_head: 8
scale_attn_by_inverse_layer_idx: false
rotary_emb_fraction: 0.25
use_flash_attn: false
fused_mlp: true
fused_bias_fc: false
fused_dropout_add_ln: false
pad_vocab_size_multiple: 8
bos_token_id: 1
eos_token_id: 2     # cm
pad_token_id: 69120 # cm
# torch_dtype: auto # bfloat16
max_length: 1024 # 同时会设置data_config.yaml的seq_length

# 这里设置true是断开后复训
resume_from_checkpoint: true